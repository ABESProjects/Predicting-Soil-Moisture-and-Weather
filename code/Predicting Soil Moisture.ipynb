{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project\n",
    "\n",
    "# Predicting Soil Moisture using Weather Data\n",
    "\n",
    "# CS-A (ABESEC)\n",
    "\n",
    "# Atul Dagar (2000320120056)\n",
    "# Anurag Bhardwaj (2000320120039)\n",
    "# Aryan Tyagi (2000320120050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os as os\n",
    "from time import time, strftime\n",
    "from datetime import datetime\n",
    "from cloudmesh.common.StopWatch import StopWatch\n",
    "from cloudmesh.common.Benchmark import Benchmark\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score, r2_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.3\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+------------------+--------------------------------------------------------------------+\n",
      "| Attribute        | Value                                                              |\n",
      "|------------------+--------------------------------------------------------------------|\n",
      "| cpu              |                                                                    |\n",
      "| cpu_cores        | 4                                                                  |\n",
      "| cpu_count        | 8                                                                  |\n",
      "| cpu_threads      | 8                                                                  |\n",
      "| frequency        | scpufreq(current=1600.0, min=0.0, max=1800.0)                      |\n",
      "| mem.available    | 3.2 GiB                                                            |\n",
      "| mem.free         | 3.2 GiB                                                            |\n",
      "| mem.percent      | 59.8 %                                                             |\n",
      "| mem.total        | 7.9 GiB                                                            |\n",
      "| mem.used         | 4.7 GiB                                                            |\n",
      "| platform.version | ('10', '10.0.19041', 'SP0', 'Multiprocessor Free')                 |\n",
      "| python           | 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)] |\n",
      "| python.pip       | 20.1.1                                                             |\n",
      "| python.version   | 3.8.3                                                              |\n",
      "| sys.platform     | win32                                                              |\n",
      "| uname.machine    | AMD64                                                              |\n",
      "| uname.node       | LAPTOP-LDTH8KS9                                                    |\n",
      "| uname.processor  | Intel64 Family 6 Model 142 Stepping 10, GenuineIntel               |\n",
      "| uname.release    | 10                                                                 |\n",
      "| uname.system     | Windows                                                            |\n",
      "| uname.version    | 10.0.19041                                                         |\n",
      "| user             | sanjay kumar                                                       |\n",
      "+------------------+--------------------------------------------------------------------+\n",
      "\n",
      "+--------+----------+--------+-------+---------------------+-------+-------+-----------------+--------------+---------+----------------------------------------------------+\n",
      "| Name   | Status   |   Time |   Sum | Start               | tag   | msg   | Node            | User         | OS      | Version                                            |\n",
      "|--------+----------+--------+-------+---------------------+-------+-------+-----------------+--------------+---------+----------------------------------------------------|\n",
      "| a      | ok       |  3.007 | 3.007 | 2021-12-22 09:46:06 |       |       | LAPTOP-LDTH8KS9 | sanjay kumar | Windows | ('10', '10.0.19041', 'SP0', 'Multiprocessor Free') |\n",
      "+--------+----------+--------+-------+---------------------+-------+-------+-----------------+--------------+---------+----------------------------------------------------+\n",
      "\n",
      "# csv,timer,status,time,sum,start,tag,msg,uname.node,user,uname.system,platform.version\n",
      "# csv,a,ok,3.007,3.007,2021-12-22 09:46:06,,None,LAPTOP-LDTH8KS9,sanjay kumar,Windows,('10', '10.0.19041', 'SP0', 'Multiprocessor Free')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "StopWatch.start(\"a\")\n",
    "time.sleep(3)\n",
    "StopWatch.stop(\"a\")\n",
    "StopWatch.status(\"a\", True)\n",
    "StopWatch.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b():\n",
    "  Benchmark.Start()\n",
    "  print (\"b\")\n",
    "  import time\n",
    "  time.sleep(3)\n",
    "  Benchmark.Stop()\n",
    "\n",
    "def c():\n",
    "  Benchmark.Start()\n",
    "  print (\"c\")\n",
    "  import time\n",
    "  time.sleep(1)\n",
    "  Benchmark.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "b()\n",
    "c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+------------------+--------------------------------------------------------------------+\n",
      "| Attribute        | Value                                                              |\n",
      "|------------------+--------------------------------------------------------------------|\n",
      "| cpu              |                                                                    |\n",
      "| cpu_cores        | 4                                                                  |\n",
      "| cpu_count        | 8                                                                  |\n",
      "| cpu_threads      | 8                                                                  |\n",
      "| frequency        | scpufreq(current=1600.0, min=0.0, max=1800.0)                      |\n",
      "| mem.available    | 3.2 GiB                                                            |\n",
      "| mem.free         | 3.2 GiB                                                            |\n",
      "| mem.percent      | 59.5 %                                                             |\n",
      "| mem.total        | 7.9 GiB                                                            |\n",
      "| mem.used         | 4.7 GiB                                                            |\n",
      "| platform.version | ('10', '10.0.19041', 'SP0', 'Multiprocessor Free')                 |\n",
      "| python           | 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)] |\n",
      "| python.pip       | 20.1.1                                                             |\n",
      "| python.version   | 3.8.3                                                              |\n",
      "| sys.platform     | win32                                                              |\n",
      "| uname.machine    | AMD64                                                              |\n",
      "| uname.node       | LAPTOP-LDTH8KS9                                                    |\n",
      "| uname.processor  | Intel64 Family 6 Model 142 Stepping 10, GenuineIntel               |\n",
      "| uname.release    | 10                                                                 |\n",
      "| uname.system     | Windows                                                            |\n",
      "| uname.version    | 10.0.19041                                                         |\n",
      "| user             | sanjay kumar                                                       |\n",
      "+------------------+--------------------------------------------------------------------+\n",
      "\n",
      "+----------------------------------+----------+--------+-------+---------------------+-------+-------+-----------------+--------------+---------+----------------------------------------------------+\n",
      "| Name                             | Status   |   Time |   Sum | Start               | tag   | msg   | Node            | User         | OS      | Version                                            |\n",
      "|----------------------------------+----------+--------+-------+---------------------+-------+-------+-----------------+--------------+---------+----------------------------------------------------|\n",
      "| a                                | ok       |  3.007 | 3.007 | 2021-12-22 09:46:06 |       |       | LAPTOP-LDTH8KS9 | sanjay kumar | Windows | ('10', '10.0.19041', 'SP0', 'Multiprocessor Free') |\n",
      "| <ipython-input-4-124396bcabb3>/b | ok       |  3.003 | 3.003 | 2021-12-22 09:46:12 |       |       | LAPTOP-LDTH8KS9 | sanjay kumar | Windows | ('10', '10.0.19041', 'SP0', 'Multiprocessor Free') |\n",
      "| <ipython-input-4-124396bcabb3>/c | ok       |  1.014 | 1.014 | 2021-12-22 09:46:15 |       |       | LAPTOP-LDTH8KS9 | sanjay kumar | Windows | ('10', '10.0.19041', 'SP0', 'Multiprocessor Free') |\n",
      "+----------------------------------+----------+--------+-------+---------------------+-------+-------+-----------------+--------------+---------+----------------------------------------------------+\n",
      "\n",
      "# csv,timer,status,time,sum,start,tag,msg,uname.node,user,uname.system,platform.version\n",
      "# csv,a,ok,3.007,3.007,2021-12-22 09:46:06,,None,LAPTOP-LDTH8KS9,sanjay kumar,Windows,('10', '10.0.19041', 'SP0', 'Multiprocessor Free')\n",
      "# csv,<ipython-input-4-124396bcabb3>/b,ok,3.003,3.003,2021-12-22 09:46:12,,None,LAPTOP-LDTH8KS9,sanjay kumar,Windows,('10', '10.0.19041', 'SP0', 'Multiprocessor Free')\n",
      "# csv,<ipython-input-4-124396bcabb3>/c,ok,1.014,1.014,2021-12-22 09:46:15,,None,LAPTOP-LDTH8KS9,sanjay kumar,Windows,('10', '10.0.19041', 'SP0', 'Multiprocessor Free')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Benchmark.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_Data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "        self.weather_dir = ''\n",
    "        self.soil_dir = ''\n",
    "        self.drop_columns = ['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'AWND_ATTRIBUTES', 'PGTM_ATTRIBUTES', \n",
    "                             'PSUN', 'PSUN_ATTRIBUTES', 'SNOW', 'SNOW_ATTRIBUTES', 'SNWD', 'SNWD_ATTRIBUTES', 'TAVG',\n",
    "                             'TAVG_ATTRIBUTES', 'TMAX_ATTRIBUTES', 'TMIN_ATTRIBUTES', 'TSUN', 'TSUN_ATTRIBUTES', 'WDF2_ATTRIBUTES', \n",
    "                             'WDF5_ATTRIBUTES', 'WSF2_ATTRIBUTES','WSF5_ATTRIBUTES', 'WT01_ATTRIBUTES', 'WT02_ATTRIBUTES', \n",
    "                             'WT03_ATTRIBUTES', 'WT06_ATTRIBUTES', 'WT08_ATTRIBUTES', 'PRCP_ATTRIBUTES']\n",
    "        \n",
    "    def fit(self, w_dir, s_dir):\n",
    "        self.weather_dir = w_dir\n",
    "        self.soil_dir = s_dir\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #Aggregate all 43 files into one file\n",
    "        file_list = os.listdir(self.soil_dir)\n",
    "        agg_data = pd.DataFrame()\n",
    "        for file in file_list:\n",
    "            path = self.soil_dir + file\n",
    "            curr_data = pd.read_csv(path, sep='\\t')\n",
    "            agg_data = agg_data.append(curr_data)\n",
    "        \n",
    "        #Drop rows with only NAs for measurement values\n",
    "        soil = agg_data.dropna(thresh=10)\n",
    "        \n",
    "        #Import weather files and drop unnessecary fields\n",
    "        weather = pd.read_csv(self.weather_dir)\n",
    "        drop_cols = list(set(weather.columns).intersection(self.drop_columns))\n",
    "        weather = weather.drop(columns = self.drop_columns)\n",
    "        \n",
    "        #Convert both files to use same datetime\n",
    "        soil['Date'] = pd.to_datetime(soil['Date'])\n",
    "        weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "        \n",
    "        #Join previous 16 days weather to moisture readings\n",
    "        for i in range(0, 17):\n",
    "            weather_new = weather.add_suffix('_' + str(i))\n",
    "            soil = soil.merge(weather_new, how = 'left', left_on = 'Date', right_on = weather['DATE'] - pd.DateOffset(i * -1))\n",
    "            \n",
    "        #Store the month of the reading as a feature\n",
    "        soil['Month'] = pd.DatetimeIndex(soil['Date']).month\n",
    "        \n",
    "        date_attribs = ['Date', 'DATE_0', 'DATE_1', 'DATE_2', 'DATE_3', 'DATE_4','DATE_5', 'DATE_6', 'DATE_7', 'DATE_8', 'DATE_9', 'DATE_10', \\\n",
    "                        'DATE_11', 'DATE_12', 'DATE_13', 'DATE_14', 'DATE_15', 'DATE_16']\n",
    "        \n",
    "        if 'DATE_0' in list(soil.columns):\n",
    "            soil.drop(columns = date_attribs, inplace = True)\n",
    "        soil['Location'] = soil['Location'].astype('object')\n",
    "            \n",
    "        return soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Engineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #Add categorical feature that simply stores if it rained that day or not\n",
    "        for i in range(17):\n",
    "            col_name = 'PRCP_' + str(i)\n",
    "            rain_y_n_name = 'RAIN_Y_N_' + str(i)\n",
    "            X[rain_y_n_name] = np.nan\n",
    "            X[rain_y_n_name].loc[X[col_name] > 0] = 1\n",
    "            X[rain_y_n_name].loc[X[col_name] == 0] = 0\n",
    "            X[rain_y_n_name] = X[rain_y_n_name].astype('object')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(X)\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert_Date(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names = None):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X['Date'] = pd.to_timedelta(X['Date']).dt.total_seconds().astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "soil_file_dir = '../data/soil/'\n",
    "weather_file_dir = '../data/weather/weather_data.csv'\n",
    "x = 0\n",
    "\n",
    "pre_work_pipeline = Pipeline([\n",
    "    ('prework', Load_Data()),\n",
    "    ('features', Feature_Engineer())\n",
    "])\n",
    "\n",
    "pre_work_pipeline.fit(weather_file_dir, soil_file_dir)\n",
    "prework_df = pre_work_pipeline.transform(x)\n",
    "#Save to CSV so that we do not need to import and clean data everytime\n",
    "prework_df.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Classifier Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['VW_30cm', 'VW_60cm', 'VW_90cm', 'VW_120cm', 'VW_150cm']\n",
    "\n",
    "for cols in y_cols:\n",
    "    name = cols[3:] + '_class'\n",
    "    prework_df[name] = ''\n",
    "    prework_df[name].loc[(prework_df[cols] <= 0.1)] = '0.1'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.1) & (prework_df[cols] <= 0.2)] = '0.2'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.2) & (prework_df[cols] <= 0.3)] = '0.3'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.3) & (prework_df[cols] <= 0.4)] = '0.4'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.4) & (prework_df[cols] <= 0.5)] = '0.5'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.5) & (prework_df[cols] <= 0.6)] = '0.6'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.6) & (prework_df[cols] <= 0.7)] = '0.7'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.7) & (prework_df[cols] <= 0.8)] = '0.8'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.8)] = '0.9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Frames for Each Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moisture data is taken at various depths. We want to build models seperately for different depths. So we need to make a dataframe for each depth so that we can elminate entire rows where the predictor is NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split out y values\n",
    "all_y_cols = ['VW_30cm', 'VW_60cm', 'VW_90cm', 'VW_120cm', 'VW_150cm', '30cm_class', '60cm_class', '90cm_class', '120cm_class', '150cm_class']\n",
    "X_sets = {}\n",
    "y_sets = {}\n",
    "x_cols = [col for col in prework_df.columns if col not in y_cols]\n",
    "X = prework_df.loc[:, x_cols]\n",
    "#y = prework_df.loc[:, y_cols]\n",
    "\n",
    "for cols in all_y_cols:\n",
    "    if cols[:1] == 'V':\n",
    "        dataset_name = cols[3:]\n",
    "    else:\n",
    "        dataset_name = cols\n",
    "    holder = prework_df.dropna(subset = [cols])\n",
    "    X_sets[dataset_name] = holder[x_cols].fillna(0)\n",
    "    y_sets[dataset_name] = holder[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test data\n",
    "# 80-20 ratio\n",
    "# Trying to keep same ratios for each location using stratify\n",
    "# Could have done this in the cell above, but wanted a seperate step for this\n",
    "X_train_set = {}\n",
    "X_test_set = {}\n",
    "y_train_set = {}\n",
    "y_test_set = {}\n",
    "\n",
    "for cols in all_y_cols:\n",
    "    if cols[:1] == 'V':\n",
    "        dataset_name = cols[3:]\n",
    "    else:\n",
    "        dataset_name = cols \n",
    "    X_train_set[dataset_name], X_test_set[dataset_name], y_train_set[dataset_name], y_test_set[dataset_name] = train_test_split(X_sets[dataset_name], y_sets[dataset_name], \\\n",
    "                                                                                                                                test_size=0.2, stratify = X_sets[dataset_name]['Location'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = X_train_set['60cm'].select_dtypes(exclude=['object', 'category']).columns\n",
    "cat_attribs = X_train_set['60cm'].select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = '')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_attribs),\n",
    "        ('cat', categorical_transformer, cat_attribs)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Experiment  Depth   Fit_Time  Pred_Time      r2_score  \\\n",
      "0  First Linear Reg   30cm  32.042364  14.371771  9.154623e-01   \n",
      "1  First Linear Reg   60cm   2.874317   0.135288 -1.662894e+14   \n",
      "2  First Linear Reg   90cm   2.858727   0.156215  9.487954e-01   \n",
      "3  First Linear Reg  120cm   2.874285   0.171831  9.460321e-01   \n",
      "4  First Linear Reg  150cm   2.952462   0.125001  9.433287e-01   \n",
      "\n",
      "              datetime  \n",
      "0  2021-12-22 15:17:55  \n",
      "1  2021-12-22 15:17:58  \n",
      "2  2021-12-22 15:18:05  \n",
      "3  2021-12-22 15:18:08  \n",
      "4  2021-12-22 15:18:11  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', LinearRegression())])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time.time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time.time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time.time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['First Linear Reg', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great Scores, but oddly 60 cm has a very small r2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Experiment  Depth   Fit_Time  Pred_Time      r2_score  \\\n",
      "0       First Linear Reg   30cm  32.042364  14.371771  9.154623e-01   \n",
      "1       First Linear Reg   60cm   2.874317   0.135288 -1.662894e+14   \n",
      "2       First Linear Reg   90cm   2.858727   0.156215  9.487954e-01   \n",
      "3       First Linear Reg  120cm   2.874285   0.171831  9.460321e-01   \n",
      "4       First Linear Reg  150cm   2.952462   0.125001  9.433287e-01   \n",
      "5  Ridge Reg - Alpha = 1   30cm  28.188629   0.140590  9.162112e-01   \n",
      "6  Ridge Reg - Alpha = 1   60cm   1.312190   0.218699  9.427566e-01   \n",
      "7  Ridge Reg - Alpha = 1   90cm   1.202838   0.156215  9.487904e-01   \n",
      "8  Ridge Reg - Alpha = 1  120cm   1.140353   0.140597  9.460320e-01   \n",
      "9  Ridge Reg - Alpha = 1  150cm   1.140351   0.140591  9.433202e-01   \n",
      "\n",
      "              datetime  \n",
      "0  2021-12-22 15:17:55  \n",
      "1  2021-12-22 15:17:58  \n",
      "2  2021-12-22 15:18:05  \n",
      "3  2021-12-22 15:18:08  \n",
      "4  2021-12-22 15:18:11  \n",
      "5  2021-12-22 15:18:39  \n",
      "6  2021-12-22 15:18:41  \n",
      "7  2021-12-22 15:18:42  \n",
      "8  2021-12-22 15:18:43  \n",
      "9  2021-12-22 15:18:45  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', Ridge(alpha = 1))])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time.time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time.time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time.time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['Ridge Reg - Alpha = 1', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results are better! Let's try Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Experiment  Depth   Fit_Time  Pred_Time      r2_score  \\\n",
      "0        First Linear Reg   30cm  32.042364  14.371771  9.154623e-01   \n",
      "1        First Linear Reg   60cm   2.874317   0.135288 -1.662894e+14   \n",
      "2        First Linear Reg   90cm   2.858727   0.156215  9.487954e-01   \n",
      "3        First Linear Reg  120cm   2.874285   0.171831  9.460321e-01   \n",
      "4        First Linear Reg  150cm   2.952462   0.125001  9.433287e-01   \n",
      "5   Ridge Reg - Alpha = 1   30cm  28.188629   0.140590  9.162112e-01   \n",
      "6   Ridge Reg - Alpha = 1   60cm   1.312190   0.218699  9.427566e-01   \n",
      "7   Ridge Reg - Alpha = 1   90cm   1.202838   0.156215  9.487904e-01   \n",
      "8   Ridge Reg - Alpha = 1  120cm   1.140353   0.140597  9.460320e-01   \n",
      "9   Ridge Reg - Alpha = 1  150cm   1.140351   0.140591  9.433202e-01   \n",
      "10  Lasso Reg - Alpha = 1   30cm  15.871741   0.156247 -1.832157e-04   \n",
      "11  Lasso Reg - Alpha = 1   60cm   1.327864   0.140546 -4.613909e-05   \n",
      "12  Lasso Reg - Alpha = 1   90cm   1.359043   0.140626 -5.673799e-06   \n",
      "13  Lasso Reg - Alpha = 1  120cm   1.373374   0.140591 -1.131381e-06   \n",
      "14  Lasso Reg - Alpha = 1  150cm   1.348597   0.140538 -1.814059e-04   \n",
      "\n",
      "               datetime  \n",
      "0   2021-12-22 15:17:55  \n",
      "1   2021-12-22 15:17:58  \n",
      "2   2021-12-22 15:18:05  \n",
      "3   2021-12-22 15:18:08  \n",
      "4   2021-12-22 15:18:11  \n",
      "5   2021-12-22 15:18:39  \n",
      "6   2021-12-22 15:18:41  \n",
      "7   2021-12-22 15:18:42  \n",
      "8   2021-12-22 15:18:43  \n",
      "9   2021-12-22 15:18:45  \n",
      "10  2021-12-22 15:19:01  \n",
      "11  2021-12-22 15:19:02  \n",
      "12  2021-12-22 15:19:04  \n",
      "13  2021-12-22 15:19:05  \n",
      "14  2021-12-22 15:19:07  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', Lasso(alpha = 1))])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time.time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time.time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time.time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['Lasso Reg - Alpha = 1', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At least with with these parameters, Lasso Fits Poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge with a built in gridsearch cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Experiment  Depth   Fit_Time  Pred_Time      r2_score  \\\n",
      "0        First Linear Reg   30cm  32.042364  14.371771  9.154623e-01   \n",
      "1        First Linear Reg   60cm   2.874317   0.135288 -1.662894e+14   \n",
      "2        First Linear Reg   90cm   2.858727   0.156215  9.487954e-01   \n",
      "3        First Linear Reg  120cm   2.874285   0.171831  9.460321e-01   \n",
      "4        First Linear Reg  150cm   2.952462   0.125001  9.433287e-01   \n",
      "5   Ridge Reg - Alpha = 1   30cm  28.188629   0.140590  9.162112e-01   \n",
      "6   Ridge Reg - Alpha = 1   60cm   1.312190   0.218699  9.427566e-01   \n",
      "7   Ridge Reg - Alpha = 1   90cm   1.202838   0.156215  9.487904e-01   \n",
      "8   Ridge Reg - Alpha = 1  120cm   1.140353   0.140597  9.460320e-01   \n",
      "9   Ridge Reg - Alpha = 1  150cm   1.140351   0.140591  9.433202e-01   \n",
      "10  Lasso Reg - Alpha = 1   30cm  15.871741   0.156247 -1.832157e-04   \n",
      "11  Lasso Reg - Alpha = 1   60cm   1.327864   0.140546 -4.613909e-05   \n",
      "12  Lasso Reg - Alpha = 1   90cm   1.359043   0.140626 -5.673799e-06   \n",
      "13  Lasso Reg - Alpha = 1  120cm   1.373374   0.140591 -1.131381e-06   \n",
      "14  Lasso Reg - Alpha = 1  150cm   1.348597   0.140538 -1.814059e-04   \n",
      "15       Ridge Reg - GSCV   30cm  15.183893   0.187457  9.162351e-01   \n",
      "16       Ridge Reg - GSCV   60cm   5.366714   0.156221  9.427570e-01   \n",
      "17       Ridge Reg - GSCV   90cm   5.436197   0.140594  9.487957e-01   \n",
      "18       Ridge Reg - GSCV  120cm   5.483074   0.171830  9.460322e-01   \n",
      "19       Ridge Reg - GSCV  150cm   5.561178   0.203086  9.433280e-01   \n",
      "\n",
      "               datetime  \n",
      "0   2021-12-22 15:17:55  \n",
      "1   2021-12-22 15:17:58  \n",
      "2   2021-12-22 15:18:05  \n",
      "3   2021-12-22 15:18:08  \n",
      "4   2021-12-22 15:18:11  \n",
      "5   2021-12-22 15:18:39  \n",
      "6   2021-12-22 15:18:41  \n",
      "7   2021-12-22 15:18:42  \n",
      "8   2021-12-22 15:18:43  \n",
      "9   2021-12-22 15:18:45  \n",
      "10  2021-12-22 15:19:01  \n",
      "11  2021-12-22 15:19:02  \n",
      "12  2021-12-22 15:19:04  \n",
      "13  2021-12-22 15:19:05  \n",
      "14  2021-12-22 15:19:07  \n",
      "15  2021-12-22 15:19:22  \n",
      "16  2021-12-22 15:19:28  \n",
      "17  2021-12-22 15:19:33  \n",
      "18  2021-12-22 15:19:39  \n",
      "19  2021-12-22 15:19:45  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', RidgeCV(alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]))])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time.time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time.time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time.time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['Ridge Reg - GSCV', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch found alpha = 1 to be the best parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Regressor Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now Ridge Regression with an alpha of 1 is winning as the best model so far. Let's see if we can beat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Experiment  Depth    Fit_Time  Pred_Time  r2_score  \\\n",
      "0  Random Forest - Default   30cm  688.237841   0.860152  0.980310   \n",
      "1  Random Forest - Default   60cm  680.878078   0.687300  0.990726   \n",
      "2  Random Forest - Default   90cm  689.800418   0.734206  0.992370   \n",
      "3  Random Forest - Default  120cm  722.545116   0.718582  0.992590   \n",
      "4  Random Forest - Default  150cm  733.399503   0.725267  0.993203   \n",
      "\n",
      "              datetime  \n",
      "0  2021-12-22 15:31:14  \n",
      "1  2021-12-22 15:42:35  \n",
      "2  2021-12-22 15:54:06  \n",
      "3  2021-12-22 16:06:09  \n",
      "4  2021-12-22 16:18:23  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', RandomForestRegressor())])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log_other\n",
    "except NameError:\n",
    "    log_other = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "for cols in data_cols:\n",
    "    t0 = time.time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time.time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time.time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_other.loc[len(log_other)] = ['Random Forest - Default', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing results! Although it takes considerably longer to train, the default does rather well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a litmus test, lets just try a few more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Experiment  Depth    Fit_Time  Pred_Time  r2_score  \\\n",
      "0  Random Forest - Default   30cm  688.237841   0.860152  0.980310   \n",
      "1  Random Forest - Default   60cm  680.878078   0.687300  0.990726   \n",
      "2  Random Forest - Default   90cm  689.800418   0.734206  0.992370   \n",
      "3  Random Forest - Default  120cm  722.545116   0.718582  0.992590   \n",
      "4  Random Forest - Default  150cm  733.399503   0.725267  0.993203   \n",
      "5            SVM - Default   30cm   63.639315   7.706923  0.658935   \n",
      "6            SVM - Default   60cm  148.874223  10.001588  0.753807   \n",
      "7            SVM - Default   90cm  150.792928  10.414850  0.775367   \n",
      "8            SVM - Default  120cm  127.845249   9.556673  0.746775   \n",
      "9            SVM - Default  150cm  158.235881  11.079853  0.747956   \n",
      "\n",
      "              datetime  \n",
      "0  2021-12-22 15:31:14  \n",
      "1  2021-12-22 15:42:35  \n",
      "2  2021-12-22 15:54:06  \n",
      "3  2021-12-22 16:06:09  \n",
      "4  2021-12-22 16:18:23  \n",
      "5  2021-12-22 16:19:35  \n",
      "6  2021-12-22 16:22:14  \n",
      "7  2021-12-22 16:24:55  \n",
      "8  2021-12-22 16:27:12  \n",
      "9  2021-12-22 16:30:02  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', SVR())])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log_other\n",
    "except NameError:\n",
    "    log_other = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "for cols in data_cols:\n",
    "    t0 = time.time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time.time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time.time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_other.loc[len(log_other)] = ['SVM - Default', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just with the default values, SVM, did not perform well, but this could just mean that default parameters are not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Experiment  Depth    Fit_Time  Pred_Time  r2_score  \\\n",
      "0   Random Forest - Default   30cm  688.237841   0.860152  0.980310   \n",
      "1   Random Forest - Default   60cm  680.878078   0.687300  0.990726   \n",
      "2   Random Forest - Default   90cm  689.800418   0.734206  0.992370   \n",
      "3   Random Forest - Default  120cm  722.545116   0.718582  0.992590   \n",
      "4   Random Forest - Default  150cm  733.399503   0.725267  0.993203   \n",
      "5             SVM - Default   30cm   63.639315   7.706923  0.658935   \n",
      "6             SVM - Default   60cm  148.874223  10.001588  0.753807   \n",
      "7             SVM - Default   90cm  150.792928  10.414850  0.775367   \n",
      "8             SVM - Default  120cm  127.845249   9.556673  0.746775   \n",
      "9             SVM - Default  150cm  158.235881  11.079853  0.747956   \n",
      "10            SGD - Default   30cm    6.263381   0.558115  0.889629   \n",
      "11            SGD - Default   60cm    1.503087   0.148150  0.930496   \n",
      "12            SGD - Default   90cm    1.475703   0.139627  0.941424   \n",
      "13            SGD - Default  120cm    1.440918   0.169494  0.935683   \n",
      "14            SGD - Default  150cm   29.228487   0.136069  0.928644   \n",
      "\n",
      "               datetime  \n",
      "0   2021-12-22 15:31:14  \n",
      "1   2021-12-22 15:42:35  \n",
      "2   2021-12-22 15:54:06  \n",
      "3   2021-12-22 16:06:09  \n",
      "4   2021-12-22 16:18:23  \n",
      "5   2021-12-22 16:19:35  \n",
      "6   2021-12-22 16:22:14  \n",
      "7   2021-12-22 16:24:55  \n",
      "8   2021-12-22 16:27:12  \n",
      "9   2021-12-22 16:30:02  \n",
      "10  2021-12-22 16:30:11  \n",
      "11  2021-12-22 16:30:13  \n",
      "12  2021-12-22 16:30:14  \n",
      "13  2021-12-22 16:30:16  \n",
      "14  2021-12-22 16:30:45  \n"
     ]
    }
   ],
   "source": [
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', SGDRegressor())])\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log_other\n",
    "except NameError:\n",
    "    log_other = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'datetime'])\n",
    "for cols in data_cols:\n",
    "    t0 = time.time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time.time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time.time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_other.loc[len(log_other)] = ['SGD - Default', cols, t1-t0, t2-t1, r2sc, now]\n",
    "    \n",
    "print(log_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following, will take a considerable amount of time to run. Run with caution!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment is not included in the final report, but shows an extension of trying to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  7.9min\n"
     ]
    }
   ],
   "source": [
    "## Param grid comes from the following site:\n",
    "## https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', RandomForestRegressor())])\n",
    "\n",
    "param_grid = {'classifier__bootstrap': [True, False],\n",
    "              'classifier__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "              'classifier__max_features': ['auto', 'sqrt'],\n",
    "              'classifier__min_samples_leaf': [1, 2, 4],\n",
    "              'classifier__min_samples_split': [2, 5, 10],\n",
    "              'classifier__n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "\n",
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "cv_res = {}\n",
    "try:\n",
    "    log_rf\n",
    "except NameError:\n",
    "    log_rf = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'best_params' 'datetime'])\n",
    "for cols in data_cols:\n",
    "    t0 = time.time()\n",
    "    random_search = RandomizedSearchCV(estimator = pipe_with_estimator, param_distributions = param_grid, n_iter = 10, cv = 3, verbose=10, random_state=42, n_jobs = -1)\n",
    "    random_search.fit(X_train_set[cols], y_train_set[cols])\n",
    "    best = random_search.best_params_\n",
    "    t1 = time.time()\n",
    "    preds = random_search.predict(X_test_set[cols])\n",
    "    t2 = time.time()\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_rf.loc[len(log_rf)] = ['RF - random search', cols, t1-t0, t2-t1, r2sc, best, now]\n",
    "    cv_res[cols] = random_search.cv_results_\n",
    "    print(log_rf)\n",
    "    \n",
    "print(log_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
